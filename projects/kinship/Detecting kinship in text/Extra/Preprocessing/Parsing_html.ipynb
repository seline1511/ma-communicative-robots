{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML\n",
    "This notebook converts the HTML Friends transcripts found at https://fangj.github.io/friends/ into a Python-readable format.\n",
    "This notebook outputs a single csv file containing each utterance (that could be parsed, including 166 out of 236 total episodes) along with some meta-data. This csv file is in the same format as the MELD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# https://fangj.github.io/friends/\n",
    "# https://fangj.github.io/friends/season/SSEE.html\n",
    "'''\n",
    "filepath = \"../Friends_html_data/S4E20.html\"\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as infile:\n",
    "    soup = BeautifulSoup(infile, \"html.parser\")\n",
    "'''\n",
    "url = \"https://fangj.github.io/friends/season/0101.html\"\n",
    "r = requests.get(url)\n",
    "# check r.status_code == 200\n",
    "soup = BeautifulSoup(r.text, \"html.parser\") # r.text is an html string in this case, so pass directly to BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = soup.find_all(\"p\")\n",
    "for line in texts:\n",
    "    parsed_line = line.get_text().replace(u'\\xa0', u' ').replace(\"\\n\", \" \")\n",
    "    #if \"(\" in parsed_line:\n",
    "    print(\"Scene\" in parsed_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dialogues(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Treat [] (and \"Commercial Break\") as breaks between dialogues (I think); also look for \"Opening Credits\"\n",
    "### Get rid of anything between () first? There's some within dialogue, \"Paul: (entering from Monica's room) Morning.\",\n",
    "### and others as description-type stuff \"(There's a knock on the door and it's Paul.)\"\n",
    "\n",
    "\n",
    "### S9E8 gets a little weird, but looks pretty uniform before that... On and after S9E8:\n",
    "### Some <> for direction type stuff,too (later seasons only?) \"Rachel: Amy! <pause> Yes I do.. I really do. <grabs   Ross' hand for support>\"\n",
    "### Also keep track of lines like \"Monica to Emma: Hey you.\", and \"Amy coming out of the bathroom: Hey. Hey where's the baby?\"\n",
    "### Also unmarked direction:\n",
    "# \"Amy: <points to Chandler> This guy? Seriously?\n",
    "# Later in the day.\n",
    "# Monica: Okay! It's time for dinner...\"\"\n",
    "### Also \"-Cut to Rachel (Phone ringing)\"\n",
    "### Also extra : \"Monica:: what's the big deal, you f...\"\n",
    "\n",
    "\n",
    "\n",
    "lines = [] \n",
    "if \"[\" in line:\n",
    "    prev_line = line[:line.index(\"[\")]\n",
    "    if prev_line!=\"\":\n",
    "        lines.append(prev_line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_bracketed_text(string):\n",
    "    '''\n",
    "    Given a string, eliminates any text between pairs of round brackets () (returns new string)\n",
    "    '''\n",
    "    new_string = \"\"\n",
    "    in_bracket = False\n",
    "    for char in string:\n",
    "        if not in_bracket:\n",
    "            if char == \"(\":\n",
    "                in_bracket = True\n",
    "            else:\n",
    "                new_string += char\n",
    "        else: #in_bracket currently\n",
    "            if char == \")\":\n",
    "                in_bracket = False\n",
    "                if len(new_string)>0 and new_string[-1]==\" \":\n",
    "                    new_string = new_string[:-1] # eliminate double space (hopefully)\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_source(string):\n",
    "    '''\n",
    "    Given a string, returns a tuple of before, after a colon character.\n",
    "    The purpose of this function is for parsing script text: anything that does not have a format\n",
    "    like \"character: utterance\" will be skipped. Parsing then only needs to be done on a) weird cases where the\n",
    "    format is like that but isn't actually dialogue (like \"[Scene: ...]\"), and b) what to do when a non-dialogue\n",
    "    string comes up (end of dialogue?)\n",
    "    '''\n",
    "    string_split = string.split(\":\")\n",
    "    if len(string_split) != 2:\n",
    "        cat = parse_non_utterance(string)\n",
    "        return cat\n",
    "    else:\n",
    "        if \"Scene\" in string or \"Written by\" in string:\n",
    "            return \"dialogue_break\"\n",
    "        utterance = string_split[1]\n",
    "        parsed_utt = cut_bracketed_text(utterance)\n",
    "        return (string_split[0], parsed_utt)\n",
    "    \n",
    "# Next function will look for categories returned:\n",
    "# if cat==\"scene_break\":\n",
    "#     perform_dialogue_break\n",
    "# else: \n",
    "#     do_nothing (discard line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_non_utterance(string):\n",
    "    '''\n",
    "    This function will take a string that does not have a single colon in it and returns\n",
    "    a 'type' according to those seen in friends files. The purpose of this is that different\n",
    "    'types' of strings require different actions. Mostly we just get rid of these strings, but\n",
    "    often they indicate a break in dialogue to be handled. We might also keep an eye out for \n",
    "    mistaken non-utterance labels (if an utterance has a colon embedded in it)\n",
    "    '''\n",
    "    cat = None\n",
    "    if \"Scene\" in string:\n",
    "        cat = \"dialogue_break\"\n",
    "    elif \"Time Lapse\" in string:\n",
    "        cat = \"dialogue_break\"\n",
    "    else:\n",
    "        cat = \"\"\n",
    "    return cat\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"(Chandler acts disgusted, but is happy that Joey has stopped snoring. However, just as he is about to leave, Joey starts snoring again. So to get him to stop, he slams the door shut, waking Joey.)\"\n",
    "string=cut_bracketed_text(string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To do:\n",
    "* Find way to work with HTML files one by one (retrieve from https://fangj.github.io/friends/)\n",
    "* For each line, find_source first, then cut_bracketed_text on utterances\n",
    "* Deal with types of returns from find_source: create dialogues (lists of source, utterance pairs), and start new dialogue when a scene_break is found\n",
    "* Analyse output, cross fingers tightly\n",
    "* If time, do stuff with post S9E7 files (okay probably don't bother with this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Format for output:\n",
    "SrNo.,Utterance,Speaker,Emotion,Sentiment,Dialogue_ID,Utterance_ID,Season,Episode,StartTime,EndTime\n",
    "1,also I was the point person on my companyÂ’s transition from the KL-5 to GR-6 system.,Chandler,neutral,neutral,0,0,8,21,\"00:16:16,059\",\"00:16:21,731\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dialogues(soup):\n",
    "    '''\n",
    "    Takes a beautiful soup html object from https://fangj.github.io/friends/\n",
    "    and extracts dialogues: lists of utterances paired with speakers\n",
    "    '''\n",
    "    dialogues = []\n",
    "    current_dialogue = []\n",
    "    texts = soup.find_all(\"p\")\n",
    "    for line in texts:\n",
    "        # gets the text of each line. If it's an utterance, adds to dialogue.\n",
    "        # If it's a dialogue break (as defined by parse_non_utterance()), finishes current\n",
    "        # dialogue and starts new one. If it's neither, just skips it.\n",
    "        parsed_line = line.get_text().replace(u'\\xa0', u' ').replace(\"\\n\", \" \")\n",
    "        return_value = find_source(parsed_line)\n",
    "        if type(return_value)==tuple: # line is an utterance, return is (speaker, utt)\n",
    "            current_dialogue.append(return_value)\n",
    "        else: # line is not dialogue\n",
    "            if return_value == \"dialogue_break\" and len(current_dialogue)>0:\n",
    "                dialogues.append(current_dialogue)\n",
    "                current_dialogue = []\n",
    "            elif return_value == \"\":\n",
    "                continue\n",
    "            else: # no other values currently valid\n",
    "                if len(current_dialogue)>0:\n",
    "                    print(f\"Error: line is not a recognized category\")\n",
    "\n",
    "    return dialogues\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{2:02d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "num_eps = [24, 24, 25, 23, 23, 24, 24, 23] # eps per season, index(+1) is season number\n",
    "seasons = list(range(1,9))\n",
    "Sr_No = 1\n",
    "Emotion = None\n",
    "Sentiment = None\n",
    "StartTime = None\n",
    "EndTime = None\n",
    "rows = []\n",
    "\n",
    "for season in seasons:\n",
    "    eps = num_eps[season-1]\n",
    "    for ep in range(1, eps+1):\n",
    "        url = f\"https://fangj.github.io/friends/season/{season:02d}{ep:02d}.html\"\n",
    "        print(f\"working on {season:02d}{ep:02d}...\")\n",
    "        tries = 0\n",
    "        while tries < 10:\n",
    "            r = requests.get(url)\n",
    "            if r.status_code == 200:\n",
    "                # request was good, move on\n",
    "                break\n",
    "            else:\n",
    "                # request was no good, try again\n",
    "                tries+=1\n",
    "        if tries >= 10:\n",
    "            print(f\"retrieving {season:02d}{ep:02d} failed. Moving on.\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        parsed_dialogues = get_dialogues(soup) # list of lists of (speaker, utterance)\n",
    "        print(f\"Num dialogues, {season}-{ep}: {len(parsed_dialogues)}\")\n",
    "        Dialogue_ID = 0\n",
    "        for dialogue in parsed_dialogues:\n",
    "            Utterance_ID = 0\n",
    "            for speaker, utterance in dialogue:\n",
    "                utt_dict = {\"Sr No.\":Sr_No, \"Utterance\":utterance, \"Speaker\":speaker, \"Emotion\":None, \"Sentiment\":None,\\\n",
    "                            \"Dialogue_ID\":Dialogue_ID, \"Utterance_ID\":Utterance_ID, \"Season\":season,\\\n",
    "                            \"Episode\":ep, \"StartTime\":None, \"EndTime\":None}\n",
    "                rows.append(utt_dict)\n",
    "                Utterance_ID += 1\n",
    "                Sr_No += 1\n",
    "            Dialogue_ID += 1\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = set()\n",
    "for row in rows:\n",
    "    season, ep, utt = row[\"Season\"], row[\"Episode\"], row[\"Utterance\"]\n",
    "    episodes.add((season, ep))\n",
    "print(sorted(episodes))\n",
    "print(len(episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('friends_html_data.csv', 'w+', encoding=\"utf-8\") as outfile:\n",
    "    headers = [\"Sr No.\",\"Utterance\",\"Speaker\",\"Emotion\",\"Sentiment\",\"Dialogue_ID\",\"Utterance_ID\",\"Season\",\\\n",
    "               \"Episode\",\"StartTime\",\"EndTime\"]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for row in rows:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML Data preprocessing\n",
    "\n",
    "This portion converts the scraped HTML Friends transcript data into a pkl file for use in our other code.\n",
    "\n",
    "This code was originally written to preprocess the MELD data, which is why we don't just save the HTML data as an appropriate pkl file in the first place.\n",
    "\n",
    "This code replaces some strangely encoded characters and saves relevant data to a pickle file, outputting this pickle file at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "\n",
    "data = \"./friends_html_data.csv\"\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "i_text = df['Utterance'].tolist()\n",
    "sources = df['Speaker'].tolist()\n",
    "seasons = df['Season'].tolist()\n",
    "episodes = df['Episode'].tolist()\n",
    "sr_numbers = df['Sr No.'].tolist()\n",
    "\n",
    "list_of_dicts = []\n",
    "i = 0\n",
    "for item in i_text:\n",
    "    if type(item)==str:\n",
    "        item2 = item.replace('\\x92', \"'\")\n",
    "        item3 = item2.replace('\\x96', \"-\")\n",
    "        item3 = item2.replace('\\x97', \"-\")\n",
    "        item4 = item3.replace('\\x85', \"...\")\n",
    "        item5 = item4.replace('\\x91', \"'\")\n",
    "        item6 = item5.replace('\\x93', \"'\")\n",
    "        item7 = item6.replace('\\x94', \"'\")\n",
    "        processed_text = item7\n",
    "        tokens = word_tokenize(processed_text)\n",
    "        processed_dict = {}\n",
    "        processed_dict['source'] = sources[i]\n",
    "        processed_dict['utt'] = processed_text\n",
    "        processed_dict['tok_utt'] = tokens\n",
    "        processed_dict['season'] = seasons[i]\n",
    "        processed_dict['episode'] = episodes[i]\n",
    "        processed_dict['sr_number'] = sr_numbers[i]\n",
    "        list_of_dicts.append(processed_dict)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"Preprocessed_html_data.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(list_of_dicts, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
